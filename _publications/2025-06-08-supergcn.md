---
title: "Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers"
collection: publications
permalink: /publication/2025-06-08-supergcn
excerpt: #'SuperGCN: efficient distributed GCN training for CPU supercomputers'
date: 2025-06-08~11
venue: 'ICS 25: Proceedings of the 39th ACM International Conference on Supercomputing'
paperurl: 'https://dl.acm.org/doi/10.1145/3721145.3730422'
citation: 'Chen Zhuang, Lingqi Zhang, Du Wu, Peng Chen, Jiajun Huang, Xin Liu, Rio Yokota, Nikoli Dryden, Toshio Endo, Satoshi Matsuoka, and Mohamed Wahib. 2025. Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers. In Proceedings of the 39th ACM International Conference on Supercomputing (ICS 25). Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3721145.3730422.'
author_profile: false
---

Training distributed full-batch GCNs on large-scale graphs suffers from inefficient memory access patterns and high communication overhead. We introduce SuperGCN, an efficient and scalable distributed GCN training framework tailored for CPU-powered supercomputers. Our contributions are threefold: (1) general and efficient aggregation operators designed for irregular memory access, (2) a hierarchical aggregation scheme that reduces communication costs without altering the graph structure, and (3) a communication-aware quantization scheme to enhance performance. SuperGCN achieves up to a sixfold speedup compared to Intel's DistGNN on Xeon-based systems, with performance scaling linearly as the number of processors increased. SuperGCN scales to a maximum of 8,192 MPI ranks on the largest publicly available dataset IGB260M, which stands as the highest scalability achieved by a full-batch GNN training system.
